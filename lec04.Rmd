---
title: "Chapter 2"
author: "DJM"
date: "4 February 2020"
output:
  pdf_document: default
  slidy_presentation:
    css: gfx/djmRslidy.css
    font_adjustment: 0
    highlight: tango
---



## What is this chapter about?

\newcommand{\Expect}[1]{\mathbb{E}\left[ #1 \right]}
\newcommand{\Var}[1]{\mathbb{V}\left[ #1 \right]}
\newcommand{\Cov}[2]{\mathrm{Cov}\left[#1,\ #2\right]}
\newcommand{\given}{\ \vert\ }



> Problems with regression, and in particular, linear regression

A quick overview:

1. The truth is almost never linear.
2. Collinearity __can__ cause difficulties for numerics and interpretation.
3. The estimator depends strongly on the marginal distribution of $X$.
4. Leaving out important variables is bad.
5. Noisy measurements of variables can be bad, but it may not matter.

## Asymptotic notation

* The Taylor series expansion of the mean function $\mu(x)$ at some point $u$
\[
\mu(x) = \mu(u) + (x-u)^{\top} \frac{\partial \mu(x)}{\partial x}\vert_{x=u} + O(\lVert x-u\rVert^2)
\]

* The notation $f(x) = O(g(x))$ means that for any $x$ there exists a constant $C$ such that $f(x)/g(x) < C$.

* More intuitively, this notation means that the remainder (all the higher order terms) are about the size of the distance between $x$ and $u$ or smaller.

* So as long as we are looking at points $u$ near by $x$, a linear approximation to $\mu(x)=\Expect{Y\given X=x}$ is reasonably accurate.

## What is bias?

* We need to be more specific about what we mean when we say __bias__.

* Bias is neither good nor bad in and of itself.

* A very simple example: let $Z_1,\ldots,Z_n \sim N(\mu, 1)$.
    
    - We don't know $\mu$, so we try to use the data (the $Z_i$'s) to estimate it.
    - I propose 3 estimators: 
        1. $\widehat{\mu}_1 = 12$, 
        2. $\widehat{\mu}_2=Z_6$, 
        3. $\widehat{\mu}_3=\overline{Z}$.
    - The __bias__ (by definition) of my estimator is $\Expect{\widehat{\mu}}-\mu$.
    - Calculate the bias and variance of each estimator.
  
## Regression in general

* If I want to predict $Y$ from $X$, it is almost always the case that
\[
\mu(x) = \Expect{Y\given X=x} \neq x^{\top}\beta
\]

* There are always those errors $O(\lVert x-u\rVert)^2$, so the __bias__ is not zero.

* We can include as many predictors as we like, but this doesn't change the fact that the world is __non-linear__.

## Covariance between the prediction error and the predictors

* In theory, we have (if we know things about the state of nature)
\[
\beta^* = \arg\min_\beta \Expect{\lVert Y-X\beta\rVert^2} = \Cov{X}{X}^{-1}\Cov{X}{Y}
\]

* Define $v^{-1}=\Cov{X}{X}^{-1}$.

* Using this optimal value $\beta^*$, what is $\Cov{Y-X\beta^*}{X}$?

\[
\begin{aligned}
\Cov{Y-X\beta^*}{X} &= \Cov{Y}{X}-\Cov{X\beta^*}{X} & \textrm{(Cov is linear)}\\
&=\Cov{Y}{X} - \Cov{X (v^{-1}\Cov{X}{Y})}{X} & \textrm{(substitute the def. of $\beta^*$)}\\
&=\Cov{Y}{X} - \Cov{X}{X}v^{-1}\Cov{X}{Y} & \textrm{(Cov is linear in the first arg)}\\
&=\Cov{Y}{X} - \Cov{X}{Y}=0.
\end{aligned}
\]


## Bias and Collinearity

* Adding or dropping variables may impact the bias of a model
  - Suppose $\mu(x)=\beta_0+\beta_1 x_1$. It __is__ linear. What is our estimator of $\beta_0$?
  - If we instead estimate the model $y_i = \beta_0$, our estimator of $\beta_0$ will be biased. How biased?
  - But now suppose that $x_1= 12$ always. Then we don't need to include $x_1$ in the model. Why not?
  - Form the matrix $[1\ x_1]$. Are the columns collinear? What does this actually mean?
  
## When two variables are collinear, a few things happen.

  1. We cannot __numerically__ calculate $(\mathbf{X}^\top\mathbf{X})^{-1}$. It is rank deficient.
  2. We cannot __intellectually__ separate the contributions of the two variables.
  3. We can (and should) drop one of them. This will not change the bias of our estimator, but it will alter our interpretations.
  4. Collinearity appears most frequently with many categorical variables.
  5. In these cases, software __automatically__ drops one of the levels resulting in the baseline case being in the intercept. Alternately, we could drop the intercept!
  6. High-dimensional problems (where we have more predictors than observations) also lead to rank deficiencies.
  6. There are methods (regularizing) which attempt to handle this issue (both the numerics and the interpretability). We may have time to cover them slightly.
  
## White noise

__White noise__ is a stronger assumption than __Gaussian__.

Consider a random vector $\epsilon$.

1. $\epsilon\sim\textrm{N}(0,\Sigma)$.
2. $\epsilon_i \sim\textrm{N}(0,\sigma^2(x_i))$.
3. $\epsilon\sim\textrm{N}(0,\sigma^2 I)$.

The third is white noise. The $\epsilon$ are normal, their variance is constant for all $i$ and independent of $x_i$, and they are independent.

## Asymptotic efficiency

> This and MLE are covered in 420.

There are many properties one can ask of estimators $\widehat{\theta}$ of parameters $\theta$

1. Unbiased: $\Expect{\widehat{\theta}}-\theta=0$
2. Consistent: $\widehat{\theta}\xrightarrow{n\rightarrow\infty} \theta$
3. Efficient: $\Var{\widehat{\theta}}$ is the smallest of all unbiased estimators
3. Asymptotically efficient: Maybe not efficient for every $n$, but in the limit, the variance is the smallest of all unbiased estimators.
4. Minimax: over all possible estimators in some class, this one has the smallest MSE for the worst problem.
5. $\ldots$

## Problems with R-squared

\[
R^2 = 1-\frac{SSE}{\sum_{i=1}^n (Y_i-\overline{Y})^2} = 1-\frac{MSE}{\frac{1}{n}\sum_{i=1}^n (Y_i-\overline{Y})^2} = 1-\frac{SSE}{SST}
\]
  
* This gets spit out by software
* $X$ and $Y$ are both normal with (empirical) correlation $r$, then $R^2=r^2$
* In this nice case, it measures how tightly grouped the data are about the regression line
* Data that are tightly grouped about the regression line can be predicted accurately by the regression line.
* Unfortunately, the implication does not go both ways.
* High $R^2$ can be achieved in many ways, same with low $R^2$
* You should just ignore it completely (and the adjusted version), and encourage your friends to do the same

## High R-squared with non-linear relationship

```{r ggplot-theme, echo=FALSE,message=FALSE}
library(tidyverse)
library(cowplot)
```

```{r nonlinear-R2,fig.align='center',fig.height=6,message=FALSE,warning=FALSE}
genY <- function(X, sig) Y = sqrt(X)+sig*rnorm(length(X))
sig=0.05; n=100
df = tibble(
  x = c(runif(n,0,1), runif(n,1,2), runif(n,10,11)),
  y = genY(x, sig),
  grp = rep(letters[1:3], each=n))

g1 = ggplot(df, aes(x, y, color=grp)) + geom_point() + 
  geom_smooth(method = 'lm', fullrange=TRUE, se = FALSE) +
  coord_cartesian(ylim=c(0,4)) + stat_function(fun=sqrt,color='black',size=2) +
  theme_cowplot()
g1
```

## What are the numbers?

```{r what-r2, fig.align='center',fig.height=6,message=FALSE,warning=FALSE}
g1
df %>% group_by(grp) %>% summarise(rsq = summary(lm(y~x))$r.sq) %>% knitr::kable(digits = 2)
```

## Anscombe's quartet

```{r anscombe}
ans = anscombe %>%
 pivot_longer(everything(),
   names_to = c(".value", "set"),
   names_pattern = "(.)(.)"
 )
ggplot(ans, aes(x,y)) + geom_point() + 
  geom_smooth(method="lm",se=FALSE,color="blue") +
  facet_wrap(~set, ncol=2) + theme_cowplot()
```

## Anscombe's quartet

```{r anscombe-summary}
ans %>% group_by(set) %>% 
  summarise(
    mx = mean(x), my = mean(y), sx = sd(x), sy = sd(y),
    int = coef(lm(y~x))[1], slope = coef(lm(y~x))[2],
    cor = cor(x,y), rsq = summary(lm(y~x))$r.sq) %>% 
  knitr::kable(digits = 2)
```
