---
title: "Chapter 3"
author: "DJM"
date: "11 February 2020"
output:
  slidy_presentation:
    css: gfx/djmRslidy.css
    font_adjustment: 0
    highlight: tango
  pdf_document: default
---

\newcommand{\Expect}[1]{\mathbb{E}\left[ #1 \right]}
\newcommand{\Var}[1]{\mathbb{V}\left[ #1 \right]}
\newcommand{\Cov}[2]{\mathrm{Cov}\left[#1,\ #2\right]}
\newcommand{\given}{\ \vert\ }
\newcommand{\E}{\mathbb{E}}
\renewcommand{\P}{\mathbb{P}}
\newcommand{\R}{\mathbb{R}}


```{r setup, echo=FALSE}
library(ggplot2)
theme_set(theme_minimal(base_family="Times"))
green = '#00AF64'
blue = '#0B61A4'
red = '#FF4900'
orange = '#FF9200'
```

# Review



## Statistical models

We observe data $Z_1,Z_2,\ldots,Z_n$ generated by some probability
distribution $P$. We want to use the data to learn about $P$. 

A __statistical model__ is a set of distributions $\P$.


Some examples:

  1. $\P = \{ 0 < p < 1 : P(z=1)=p,\ P(z=0)=1-p\}$.
  2. $\P = \{ \beta \in \R^p, \sigma>0 : Y \sim N(X^\top\beta,\sigma^2),\  X\mbox{ fixed}\}$.
  2. $\P = \{\mbox{all CDF's }F\}$.
  3. $\P = \{\mbox{all smooth functions } f: \R^p \rightarrow \R\}$
  
## Statistical models 2

We observe data $Z_1,Z_2,\ldots,Z_n$ generated by some probability
distribution $P$. We want to use the data to learn about $P$. 

$$
\P = \{ P(z=1)=p,\ P(z=0)=1-p,\ 0 < p < 1 \}
$$
  
* To completely characterize $P$, I just need to estimate $p$.

* Need to assume that $P\in\P$. 

* This assumption is mostly empty: __need independent, can't see $z=12$.__

## Statistical models 3


We observe data $Z_i=(Y_i,X_i)$ generated by some probability
distribution $P$. We want to use the data to learn about $P$. 

\[
\P = \{ \beta \in \R^p, \sigma>0 : Y \sim N(X^\top\beta,\sigma^2),\  X\mbox{ fixed}\}.
\]

  
* To completely characterize $P$, I just need to estimate $\beta$ and $\sigma$.

* Need to assume that $P\in\P$.

* This time, I have to assume a lot more: __Linearity,
    independence, Gaussian noise, no ignored variables, no collinearity, etc.__


## Convergence


Let $X_1,X_2,\ldots$ be a sequence of random variables, and let $X$ be
another random variable with distribution $P$. Let $F_n$ be the cdf of $X_n$ and let $F$ be
the cdf of $X$.

1. $X_n$ converges __in probability__ to $X$, $X_n\xrightarrow{P} X$, if for every $\epsilon>0$,
  \[
  \lim_{n\rightarrow\infty} P\left(|X_n-X| > \epsilon\right) = 0. 
  \]
  
2. $X_n$ converges __in distribution__ to $X$, $X_n\xrightarrow{D} X$, if for all $t$, 
  \[
  \lim_{n\rightarrow\infty} F_n(t) = F(t)
  \]

__Heuristics:__ the sequence is somehow getting "closer" to some limit. But things are random...



## Convergence rules


Suppose $X_1, X_2,\ldots$ are independent random variables, each
with mean $\mu$ and variance $\sigma^2$.   Let
$\overline{X}_n = \frac{1}{n}\sum_{i=1}^n X_i$.  

1. __Weak law of large numbers__
  \[
  \overline{X}_n \xrightarrow{P} \mu
  \]

  The law of large numbers tell us that the probability mass of an
  average of random variables "piles up" near its expectation. 

2. __Central limit theorem__
  \[
  \frac{\sqrt{n}(\overline{X}_n-\mu)}{\sigma}\xrightarrow{D} N(0,1).
  \]

  The CLT tells us about the shape of the "piling", when appropriately
  normalized.

## Evaluation

Once I choose some way to "learn" a statistical model, I need to
decide if I'm doing a good job.

__How do I decide if I'm doing anything good?__



## Properties

  
Lots of ways to evaluate estimators, $\widehat{\mu}$ of parameters $\mu$.

(from last time)

* Consistency: $\widehat{\mu} \xrightarrow{P} \mu$.
* Asymptotic Normality: $\widehat{\mu} \xrightarrow{D} N(\mu,\Sigma)$
* Efficiency: how large is $\Sigma$
* Unbiased: $\E[\widehat{\mu}] \overset{?}{=} \mu$
* etc.


None of these things make sense unless __your model is correct__.


##

__Your model is wrong!__


[unless you are flipping coins, gambling in a casino, or running randomized, controlled trials on cereal grains]



## Mis-specified models


What happens when your model is wrong? And it __IS__ wrong. 
  
None of those evaluation criteria make any sense. The parameters no longer have any meaning. 


[The criteria still hold in some sense: I can demand
    that I get close to the projection of the truth onto $\P$]


# Prediction

## Prediction

  
Prediction is easier: your model may not actually represent the true state of nature, but it may still
predict well.

  > Over a 13-year period, [David Leinweber] found [that] annual __butter production__ in Bangladesh "explained" 75%  of the variation in the annual returns of the Standard & Poor's 500-stock index.
    
  > By tossing in __U.S. cheese production__ and the __total population of sheep__ in both Bangladesh and the U.S., Mr. Leinweber was able to "predict" past U.S. stock returns with 99% accuracy. 

This is why we don't use $R^2$ to measure prediction accuracy.

## The setup


What do we mean by good predictions?


We make observations and then attempt to "predict" new, unobserved data.

Sometimes this is the same as estimating the mean. 
  
Mostly, we observe $(y_1,x_1),\ldots,(y_n,x_n)$, and we want some way to predict $Y$ from $X$.

## Evaluating predictions


Of course, both $Y$ and $\widehat{Y}$ are __random__

I want to know how well I can predict __on average__

Let $\widehat{f}$ be some way of making predictions $\widehat{Y}$ of $Y$ using covariates $X$

In fact, suppose I observe a dataset $\mathcal{D}_n = \{(Y_1,X_1,),\ldots,(Y_n,X_n)\}$.  

Then I want to __choose__ some $\widehat{f}$ using $\mathcal{D}_n$.

Is $\widehat{f}$ good on average?
  

## Evaluating predictions

  
Choose some __loss function__ that measures prediction quality:
  $\ell: \R\times\R\rightarrow\R$. We predict $Y$ with $\widehat{Y}$

Examples:

* __Squared-error:__   
\[\ell(y,\widehat{y}) = (y-\widehat{y})^2\]

* __Absolute-error:__  
\[\ell(y,\widehat{y}) = |y-\widehat{y}|\] 

* __Zero-One:__         
\[\ell(y,\widehat{y}) = I(y\neq\widehat{y})=\begin{cases} 0 & y=\widehat{y}\\1 & \mbox{else}\end{cases}\]
  
Can be generalized to $Y$ in arbitrary spaces.



## Prediction risk

> __Prediction risk__
  \[
  R_n(\widehat{f}) = \E[\ell(Y,\widehat{f}(X))]
  \]
  where the expectation is taken over the new data point $(Y,X)$
  and $\mathcal{D}_n$ (everything that is random).


For __regression__ applications, we will use squared-error loss:
\[
R_n(\widehat{f}) = \E[(Y-\widehat{f}(X))^2]
\]

For __classification__ applications, we will use zero-one loss:
\[
R_n(\widehat{f}) = \E[I(Y\neq\widehat f(X))]
\]



## Example 1: Estimating the mean


Suppose we know that we want to predict a quantity $Y$, where $\E[Y]= \mu \in \mathbb{R}$ and $\Var{Y} = 1$.  

That is, $Y \sim P \in \P$, where 

  \[
  \P = \{P: \E [Y] = \mu \textrm{ and } \Var{Y} = 1\}.
  \]

Our data is $\mathcal{D}_n = \{Y_1,\ldots,Y_n\}$ such that $Y_i \stackrel{i.i.d.}{\sim} P$, and we want to estimate $\mu$ (and hence $P$).


## Estimating the mean

* Let $\widehat{Y}=\overline{Y}_n$ be the sample mean.  
* We can ask about the __estimation risk__ (since we're estimating $\mu$):
  \[
  \begin{aligned}
    R_n(\overline{Y}_n; \mu) &= \E[(\overline{Y}_n-\mu)^2]\\ &= \E[\overline{Y}_n^2]
    -2\mu\E[\overline{Y}_n] + \mu^2 \\ &= \mu^2 + \frac{1}{n} - 2\mu^2 +
    \mu^2\\ &= \frac{1}{n}
  \end{aligned}
  \]



## Predicting new Y's

  
* Let $\widehat{Y}=\overline{Y}_n$ be the sample mean.

* What is the __prediction risk__ of $\overline{Y}$?

\[
    \begin{aligned}
    R_n(\overline{Y}_n) &= \E[(\overline{Y}_n-Y)^2]\\ &= \E[\overline{Y}_n^2]
    -2\E[\overline{Y}_n Y] + \E[Y^2] \\ &= \mu^2 + \frac{1}{n} - 2\mu^2 + \mu^2 +
    1 \\ &= 1 + \frac{1}{n} 
  \end{aligned}
\]

## Predicting new Y's

  
* What is the prediction risk of guessing $Y=0$?

* You can probably guess that this is a stupid idea.

* Let's show why it's stupid.

\[
      \begin{aligned}
        R_n(0) &= \E[(0-Y)^2]\\ 
                       &= 1 + \mu^2
      \end{aligned}
\]

## Predicting new Y's


What is the prediction risk of guessing $Y=\mu$?


This is a great idea, but we don't know $\mu$.

Let's see what happens anyway.

\[
      \begin{aligned}
        R_n(\mu) &= \E[(Y-\mu)^2]\\ 
                       &= 1
      \end{aligned}
\]

## Estimating the mean

  
* Prediction risk: $R(\overline{Y}_n) = 1 + \frac{1}{n}$    
* Estimation risk: $R(\overline{Y}_n;\mu) =  \frac{1}{n}$  
* There is actually a nice interpretation here:
    1. The common $1/n$ term is $\Var{\overline{Y}_n}$  
    2. The extra factor of $1$ in the prediction risk is __irreducible error__ 
        * $Y$ is a random variable, and hence noisy. 
        * We can never eliminate it's intrinsic variance.  
        * In other words, even if we knew $\mu$, we could never get closer than $1$, on average.

* Intuitively, $\overline{Y}_n$ is the obvious thing to do.


## Predicting new Y's

  
* Let's try one more: $\widehat Y_a = a\overline{Y}_n$ for some $a \in (0,1]$.
  
  \[
  R_n(\widehat Y_a) = \E[(\widehat Y_a-Y)^2] = (1 - a)^2\mu^2 +
  \frac{a^2}{n} +1 
  \]
  
* We can minimize this in $a$ to get the best possible prediction risk for an estimator of the form $\widehat Y_a$: 
  
  \[
  \arg\min_{a} R_n(\widehat Y_a) = \left(\frac{\mu^2}{\mu^2 + 1/n} \right)
  \]
  
  What happens if $\mu \ll 1$?
  
##
  
  > Wait a minute! You're saying there is a __better__ estimator than $\overline{Y}_n$?






## Bias-variance tradeoff: Estimating the mean

\[
R(a) = R_n(\widehat Y_a) = (a - 1)^2\mu^2 +  \frac{a^2}{n} + \sigma^2
\]

```{r}
mu=1; n=5; sig2=1
```

```{r, fig.align='center', echo=FALSE}
biasSqA <- function(a, mu=1) (a-1)^2 * mu
varA <- function(a, n=1) a^2/n
risk <- function(a, mu=1, n=1, sig2=1) biasSqA(a, mu) + varA(a, n) +  sig2
par(cex=1, lwd=2,mar=c(5,3,0,0))
curve(risk(x, mu=mu, n=n, sig2=sig2), from=0, to=1, col=green, las=1, bty='n', ylab='R(a)', xlab='a', ylim=c(0,2))
curve(varA(x, n=n), from=0, to=1, col=blue, add=TRUE)
curve(biasSqA(x, mu=mu), from=0, to=1, col=red, add=TRUE)
aopt = mu^2/(mu^2+1/n)
abline(v=aopt, col='grey', lwd=1)
abline(h = sig2+1/n, col='grey', lty=2)
legend(.5, 2, col=c(green,blue,red,'grey','grey'), lty=c(1,1,1,1,2), bty='n', legend=c('risk','var','bias sq',paste0('best a = ',round(aopt,3)),'risk of mean'))
```


## What?


Just to restate:

* If $\mu=$ `r mu` and $n=$ `r n` then it is better to predict with `r round(aopt,2)` $\overline{Y}_n$ than with $\overline{Y}_n$ itself.  
* In this case
  1. $R(a)=R_1(a\overline{Y}_n) =$ `r round(risk(aopt,mu,n),2)`
  2. $R(\overline{Y}_n)=$ `r 1/n+sig2`


## Prediction risk


\[
R_n(f) = \E[\ell(Y,f(X))]
\]
  
Why care about $R_n(f)$? 


* (+) Measures predictive accuracy on average.

* (+) How much confidence should you have in $f$'s predictions.

* (+) Compare with other models.

* (-) __This is hard:__  

    * Don't know $P$ (if I knew the truth, this would be easy)

  
## Risk for general models


  We just saw that when you know the true model, and you have a nice
  estimator, the prediction risk has a nice decomposition
  
  (this generalizes to much more complicated situations)

  
* Suppose we have a class of prediction functions $\mathcal{F}$,
    \[
    \textrm{e.g. }  \mathcal{F} = \left\{\beta : f(x) = x^\top \beta \right\}
    \]
* We use the data to choose some $\widehat{f}\in\mathcal{F}$ and set $\widehat{Y} =
    \widehat{f}(X)$
* The __true__ model is $g$ (not necessarily in $\mathcal F$).  Then:

  \[
    R_n(\widehat{f}) = \int \left[ \textrm{bias}^2(\widehat f(x))  + \textrm{var}(\widehat{f}(x)) \right]p(x) dx
    + \sigma^2
  \]
  where  $X \sim p$ and
  \[
    \begin{aligned}
    \textrm{bias}(\widehat f(x)) &= \E[\widehat{f}(x)] - g(x)\\
    \textrm{var}(\widehat f(x)) &= \E[ (\widehat{f}(x) - \E\widehat{f}(x))^2]\\
    \sigma^2 &= \E[(Y-g(X))^2]
  \end{aligned}
  \]

## Bias-variance decomposition


So,

1. prediction risk  =  bias$^2$  +  variance  +  irreducible error 
2. estimation risk  =  bias$^2$  +  variance
    

What is $R(a)$ for our estimator $\widehat{Y}_a=a\overline{Y}_n$?
  \[
  \begin{aligned}
  \textrm{bias}(\widehat{Y}_a) &= \E[a\overline{Y}_n] - \mu=(a-1)\mu\\
  \textrm{var}(\widehat f(x)) &= \E[ (a\overline{Y}_n - \E[ a\overline{Y}_n])^2]
  =a^2\E[(\overline{Y}_n-\mu)^2]=\frac{a^2}{n} 
  \\
  \sigma^2 &= \E[(Y-\mu)^2]=1
  \end{aligned}
  \]
  
  \[
  \left(\textrm{That is: }    R_n(\widehat{Y}_a) =  (a - 1)^2\mu^2 +
    \frac{a^2}{n} + 1\right) 
  \]
  
## Bias-variance decomposition

  
> __Important implication:__ prediction risk is proportional to estimation risk.  However, defining estimation risk requires stronger assumptions.
  

> In order to make good predictions, we want our prediction risk to be small.  This means that we want to "balance" the bias and variance.
  
##

```{r,fig.align='center',fig.height=12, fig.width=12, echo=FALSE, message=FALSE}
cols = c(blue, red, green, orange)

par(mfrow=c(2,2),bty='n',ann=FALSE,xaxt='n',yaxt='n',family='serif',mar=c(0,0,0,0),oma=c(0,2,2,0))
require(mvtnorm)
mv = matrix(c(0,0,0,0,-.5,-.5,-.5,-.5),4,byrow=T)
va = matrix(c(.01,.01,.5,.5,.05,.05,.5,.5),4,byrow=T)

for(i in 1:4){
  plot(0,0,ylim=c(-2,2),xlim=c(-2,2),pch=19,cex=70,col=blue,ann=FALSE,pty='s')
  points(0,0,pch=19,cex=50,col='white')
  points(0,0,pch=19,cex=30,col=green)
  points(0,0,pch=19,cex=10,col=orange)
  points(rmvnorm(20,mean=mv[i,],sigma=diag(va[i,])), cex=2, pch=19)
  switch(i, 
         '1'= {
           mtext('low variance',3,cex=2)
           mtext('low bias',2,cex=2)
         },
         '2'= mtext('high variance',3,cex=2),
         '3' = mtext('high bias',2,cex=2)
  )
}
```


## Bias-variance tradeoff: Overview

* __bias:__ how well does $\widehat{f}$ approximate the truth $g$
* more complicated $\mathcal{F}$, lower bias. Flexibility $\Rightarrow$ Parsimony
* more flexibility $\Rightarrow$ larger variance
* complicated models are hard to estimate precisely for fixed $n$
* irreducible error


## Example 2: Normal means


Suppose we observe the following data:
\[
Y_i = \beta_i + \epsilon_i, \quad\quad i=1,\ldots,n
\]
where $\epsilon_i\overset{iid}{\sim} \mbox{N}(0,1)$.
  
  
We want to estimate \[\boldsymbol{\beta} = (\beta_1,\ldots,\beta_n).\] 


The usual estimator (MLE) is \[\widehat{\boldsymbol{\beta}}^{MLE} = (Y_1,\ldots,Y_n).\]


This estimator has lots of nice properties: __consistent, unbiased, UMVUE, (asymptotic) normality...__

## Normal means

  
But, the standard estimator __STINKS!__ It's a bad estimator. 
  
It has no bias, but big variance.

\[
R_n(\widehat{\boldsymbol{\beta}}^{MLE}) = \mbox{bias}^2 + \mbox{var} = 0
+ n\cdot 1= n
\]

What if we use a biased estimator?

Consider the following estimator instead:
\[
\widehat{\beta}_i^S = \begin{cases} Y_i & i \in S\\ 0 & \mbox{else}. \end{cases}
\]
  
Here $S \subseteq \{1,\ldots,n\}$. 


## Normal means


What is the risk of this estimator?

\[
R_n(\widehat{\boldsymbol{\beta}}^S) = \sum_{i\not\in S} \beta_i^2 + |S|.
\]

In other words, if some $|\beta_i| < 1$, then don't bother estimating them!

In general, introduced parameters like $S$ will be called __tuning parameters__.

Of course we don't know which $|\beta_i| < 1$.

But we could try to estimate $R_n(\widehat{\boldsymbol{\beta}}^S)$, and choose $S$ to minimize our estimate.



## Estimating the risk


By definition, for any estimator $\widehat{\boldsymbol{\beta}}$, 
  
\[
R_n(\widehat{\boldsymbol{\beta}}) = 
\E\left[ \sum_{i=1}^n
    (\widehat{\beta_i}-\beta_i)^2\right]
\]

An intuitive estimator of $R_n$ is

\[
  \widehat{R}_n(\widehat{\boldsymbol{\beta}}) = \sum_{i=1}^n (\widehat{\beta_i}- Y_i)^2.
\]

This is known as the  __training error__ and it can be shown that 
\[
  \widehat{R}_n(\widehat{\boldsymbol{\beta}})  \approx R_n(\widehat{\boldsymbol{\beta}}).
\]

Also,
\[
\widehat{\boldsymbol{\beta}}^{MLE} = \arg\min_{\beta}  \widehat{R}_n(\widehat{\boldsymbol{\beta}}^{MLE}).
\]
  
What could possibly go wrong?


## Dangers of using the training error


Although
\[
  \widehat{R}_n(\widehat{\boldsymbol{\beta}})  \approx R_n(\widehat{\boldsymbol{\beta}}),
\]
this approximation can be very bad.  In fact:


__Training Error:__  $\widehat{R}_n(\widehat{\boldsymbol{\beta}}^{MLE}) = 0$

__Risk:__ $R_n(\widehat{\boldsymbol{\beta}}^{MLE}) = n$

In this case, the  __optimism__ of the training error is $n$. 


## Normal means

What about $\widehat{\boldsymbol{\beta}}^S$?

\[
  \widehat{R}_n(\widehat{\boldsymbol{\beta}}^S) = \sum_{i=1}^n (\widehat{\beta_i}-
  Y_i)^2 = \sum_{i \notin S} Y_i^2 %+ |S|\sigma^2
\]

Well
\[
  \E\left[\widehat{R}_n(\widehat{\boldsymbol{\beta}}^S)\right] =
  R_n(\widehat{\boldsymbol{\beta}}^S) - 2|S| +n.
\]

So I can choose $S$ by minimizing $\widehat{R}_n(\widehat{\boldsymbol{\beta}}^S) + 2|S|$. 
  
\[
\mbox{Estimate of Risk} = \mbox{training error} + \mbox{penalty}.
\]
  
The penalty term corrects for the optimism.

# Model selection

## Chapter 3


- Broadly, Chapter 3 is about model selection
- Choosing $S$ is model selection
- To do this, we need to estimate the Risk (MSE)
- CV can be used for this purpose 
- The training error __cannot__
- Also, you should not use `anova` or the $p$-values from the `lm` output for this purpose.
- Why? These things are to determine whether those __parameters__ are different from zero if you were to repeat the experiment many times, if the model were true, etc. etc.
- This is not the same as "are they useful for prediction = do they help me get smaller MSE"

## What is Cross Validation

- Cross validation (CV, not coefficient of variation).
- This is another way or estimating the prediction risk.
- Why?

To recap:
  
   \[
  R_n(\widehat{f}) = \E[\ell(Y,\widehat{f}(X))]
  \]
  where the expectation is taken over the new data point $(Y,X)$
  and $\mathcal{D}_n$ (everything that is random).
  
We saw one estimator of $R_n$: 
\[
\widehat{R}_n(\widehat{f}) = \sum_{i=1}^n \ell(Y_i,\widehat{f}(X_i)).
\]

This is the training error. It is a __BAD__ estimator because it is often optimistic.

## Intuition for CV


* One reason that $\widehat{R}_n(\widehat{f})$ is bad is that we are using the same data to pick $\widehat{f}$ __AND__ to estimate $R_n$.

* Notice that $R_n$ is an expected value over a __NEW__ observation $(Y,X)$.

* We don't have new data.

## Wait a minute...

...or do we?

* What if we set aside one observation, say the first one $(Y_1, X_1)$.

* We estimate $\widehat{f}^{(1)}$ without using the first observation.

* Then we test our prediction:

\[
  \widetilde{R}_1(\widehat{f}^{(1)}) = \ell(Y_1, \widehat{f}^{(1)}(X_1)).
\]

* But that was only one data point $(Y_1, X_1)$. Why stop there?

* Do the same with $(Y_2, X_2)$! Get an estimate $\widehat{f}^{(2)}$ 
without using it, then

\[
  \widetilde{R}_2(\widehat{f}^{(2)}) = \ell(Y_2, \widehat{f}^{(2)}(X_2)).
\]

## Keep going

* We can keep doing this until we try it for every data point.
* And then average them! (Averages are good)
* In the end we get
\[
\mbox{LOO-CV} = \sum_{i=1}^n \widetilde{R}_i(\widehat{f}^{(i)}) = \sum_{i=1}^n 
\ell(Y_i - \widehat{f}^{(i)}(X_i))
\]
* This is leave-one-out cross validation

## Problems with LOO-CV

1. Each held out set is small $(n=1)$. Therefore, the variance of my predictions is high.
2. Since each held out set is small, the training sets overlap. This is bad. 
    * Usually, averaging reduces variance:
    \[
    \Var{\overline{X}} = \frac{1}{n^2}\sum_{i=1}^n \Var{X_i} = \frac{1}{n}\Var{X_1}.
    \]
    * But only if the variables are independent. If not, then
    \[
    \begin{aligned}
    \Var{\overline{X}} &= \frac{1}{n^2}\Var{ \sum_{i=1}^n X_i}\\ 
      & = \frac{1}{n}\Var{X_1} + \frac{1}{n^2}\sum_{i\neq j} \Cov{X_i}{X_j}.
    \end{aligned}
    \]
    * Since the training sets overlap a lot, that covariance can be pretty big.
3. We have to estimate this model $n$ times.
    * There is an exception to this one. More on that in a minute.
  
## K-fold CV

* To aleviate some of these problems, people usually use $K$-fold cross validation.
* The idea of $K$-fold is 
  1. Divide the data into $K$ groups. 
  2. Leave a group out and estimate with the rest.
  3. Test on the held-out group. Calculate an average risk over these $\sim n/K$ data.
  4. Repeat for all $K$ groups.
  5. Average the average risks.
  
## Why K-fold better?
  1. Less overlap, smaller covariance.
  2. Larger hold-out sets, smaller variance.
  3. Less computations (only need to estimate $K$ times)

## Why might it be worse?

  1. LOO-CV is (nearly) unbiased. 
  2. The risk depends on how much data you use to estimate the model. 
  3. LOO-CV uses almost the same amount of data.
  
  
## A picture

```{r, fig.align='center',echo=FALSE}
par(mar=c(0,0,0,0))
plot(NA, NA, ylim=c(0,5), xlim=c(0,10), bty='n', yaxt='n', xaxt='n')
rect(0,.1+c(0,2,3,4),10,.9+c(0,2,3,4))
rect(c(0,1,2,9),rev(.1+c(0,2,3,4)),c(1,2,3,10),rev(.9+c(0,2,3,4)),col=red)
points(c(5,5,5),1+1:3/4,pch=19)
text(8,4.5,'Training data',cex=3)
text(2,1.5,'Testing data',cex=3,col=2)
```

## CV code

```{r}
cv.lm <- function(data, formulae, nfolds = 5) {
  data <- na.omit(data)
  formulae <- sapply(formulae, as.formula)
  responses <- sapply(formulae, function(form) all.vars(form)[1])
  names(responses) <- as.character(formulae)
  n <- nrow(data)
  fold.labels <- sample(rep(1:nfolds, length.out = n))
  mses <- matrix(NA, nrow = nfolds, ncol = length(formulae))
  colnames <- as.character(formulae)
  for (fold in 1:nfolds) {
    test.rows <- which(fold.labels == fold)
    train <- data[-test.rows, ]
    test <- data[test.rows, ]
    for (form in 1:length(formulae)) {
      current.model <- lm(formula = formulae[[form]], data = train)
      predictions <- predict(current.model, newdata = test)
      test.responses <- test[, responses[form]]
      test.errors <- test.responses - predictions
      mses[fold, form] <- mean(test.errors^2)
    }
  }
  return(colMeans(mses))
}
```

## Over-fitting vs. Under-fitting

> Over-fitting means estimating a really complicated function when you don't have enough data. 

* This is likely a low-bias/high-variance situation.

> Under-fitting means estimating a really simple function when you have lots of data. 

* This is likely a high-bias/low-variance situation.
* Both of these outcomes are bad (they have high risk).
* The best way to avoid them is to use a reasonable estimate of __prediction risk__ to choose how complicated your model should be.

## Example

```{r,fig.align='center'}
trueFunction <- function(x) sin(x) + 1/sqrt(x)
set.seed(1234)
x = runif(100, 0, 2*pi)
df1 = data.frame(x = x,
                 y = trueFunction(x) + rnorm(100, 0, .75))
library(ggplot2)
ggplot(df1, aes(x, y)) + geom_point() + xlim(0,2*pi) +
  stat_function(fun=trueFunction, color=red) 
```

##
```{r, fig.align='center', echo=TRUE, results='hide',message=FALSE}
ggplot(df1, aes(x, y)) + geom_point() + xlim(0,2*pi) +
  stat_function(fun=trueFunction, color=red) +
  geom_smooth(se=FALSE, span=1000, color=blue) + # under fit
  geom_smooth(se=FALSE, color=orange) + # not too bad
  geom_smooth(se=FALSE, span=.1, color=green) # over fit
```

---
More Questions about CV?

