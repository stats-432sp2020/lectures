---
title: "Chapter 6 in ISL: Regularization"
author: "DJM"
date: "17 April 2018"
output:
  pdf_document: default
  slidy_presentation:
    css: http://mypage.iu.edu/~dajmcdon/teaching/djmRslidy.css
    font_adjustment: 0
---

\newcommand{\Expect}[1]{\mathbb{E}\left[ #1 \right]}
\newcommand{\Var}[1]{\mathbb{V}\left[ #1 \right]}
\newcommand{\Cov}[2]{\mathrm{Cov}\left[#1,\ #2\right]}
\newcommand{\given}{\ \vert\ }
\newcommand{\E}{\mathbb{E}}
\renewcommand{\P}{\mathbb{P}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\tr}[1]{\mbox{tr}(#1)}
\newcommand{\brt}{\widehat{\beta}_{r,t}}
\newcommand{\brl}{\widehat{\beta}_{r,\lambda}}
\newcommand{\bls}{\widehat{\beta}_{ls}}
\newcommand{\blt}{\widehat{\beta}_{l,t}}
\newcommand{\bll}{\widehat{\beta}_{l,\lambda}}
\newcommand{\X}{\mathbb{X}}





## Regularization

```{r setup, echo=FALSE,results='hide',include=FALSE}
# Need the knitr package to set chunk options
library(knitr)
# Set knitr options for knitting code into the report:
# - Don't print out code (echo)
# - Save results so that code blocks aren't re-run unless code changes (cache),
# _or_ a relevant earlier code block changed (autodep), but don't re-run if the
# only thing that changed was the comments (cache.comments)
# - Don't clutter R output with messages or warnings (message, warning)
  # This _will_ leave error messages showing up in the knitted report
opts_chunk$set(message=FALSE, warning=FALSE, fig.align='center',fig.width=8,
               fig.height=4,cache=TRUE,autodep=TRUE, global.par=TRUE)
par(family = "serif", las=1, bty='n', pch=19, ann=FALSE)
library(tidyverse)
theme_set(theme_minimal(base_family="Times"))
green = '#00AF64'
blue = '#0B61A4'
red = '#FF4900'
orange = '#FF9200'
```

* Another way to control bias and variance is through ~~regularization~~ or
~~shrinkage~~.  


* Rather than selecting a few predictors that seem reasonable, maybe trying a few combinations, use them all.

* I mean ~~ALL~~.

* But, make your estimates of $\beta$ "smaller"

## Some optimization terms

* An optimization problem has 2 components:

    1. The "Objective function": e.g. $\frac{1}{n}\sum_i (y_i-x'_i \beta)^2$.
    2. The "constraint": e.g. "fewer than 5 non-zero entries in $\beta$".
    
* A constrained minimization problem is written

\[
\min_\beta f(\beta) \mbox{ subject to } C(\beta)
\]

* $f(\beta)$ is the objective function
* $C(\beta)$ is the constraint

## Regularization

One way to do this for regression is to solve (say):
\[
\begin{aligned}
\min_\beta &\frac{1}{n}\sum_i (y_i-x'_i \beta)^2 \\
\mbox{s.t.} & \sum_j \beta^2_j < t
\end{aligned}
\]
for some $t>0$.

* This is called "ridge regression".

* The ~~minimizer~~ of this problem is called $\brt$

Compare this to least squares:
\[
\begin{aligned}
\min_\beta &\frac{1}{n}\sum_i (y_i-x'_i \beta)^2 \\
\mbox{s.t.} & \beta \in \R^p
\end{aligned}
\]

## Geometry of ridge regression (2 dimensions)

```{r plotting-functions, echo=FALSE}
library(mvtnorm)
normBall <- function(q=1, len=1000){
  tg = seq(0,2*pi, length=len)
  out = data.frame(x = cos(tg)) %>%
    mutate(b=(1-abs(x)^q)^(1/q), bm=-b) %>%
    gather(key='lab', value='y',-x)
  out$lab = paste0('"||" * beta * "||"', '[',signif(q,2),']')
  return(out)
}

ellipseData <- function(n=100,xlim=c(-2,3),ylim=c(-2,3), 
                        mean=c(1,1), Sigma=matrix(c(1,0,0,.5),2)){
  df = expand.grid(x=seq(xlim[1],xlim[2],length.out = n),
                   y=seq(ylim[1],ylim[2],length.out = n))
  df$z = dmvnorm(df, mean, Sigma)
  df
}
lballmax <- function(ed,q=1,tol=1e-6){
  ed = filter(ed, x>0,y>0)
  for(i in 1:20){
    ff = abs((ed$x^q+ed$y^q)^(1/q)-1)<tol
    if(sum(ff)>0) break
    tol = 2*tol
  }
  best = ed[ff,]
  best[which.max(best$z),]
}
```

```{r,echo=FALSE}
nb = normBall(2)
ed = ellipseData()
bols = data.frame(x=1,y=1)
bhat = lballmax(ed, 2)
ggplot(nb,aes(x,y)) + xlim(-2,2) + ylim(-2,2) + geom_path(color=red) + 
  geom_contour(mapping=aes(z=z), color=blue, data=ed, bins=7) +
  geom_vline(xintercept = 0) + geom_hline(yintercept = 0) + 
  geom_point(data=bols) + coord_equal() +
  geom_label(data=bols, mapping=aes(label=bquote('hat(beta)[ls]')), parse=TRUE,
             nudge_x = .3, nudge_y = .3) +
  geom_point(data=bhat) + xlab(expression(beta[1])) + ylab(expression(beta[2])) + 
  geom_label(data=bhat, mapping=aes(label=bquote('hat(beta)[rt]')), parse=TRUE,
             nudge_x = -.4, nudge_y = -.4)
```

## Ridge regression

An equivalent way to write
\[
\brt = \arg \min_{ || \beta ||_2^2 \leq t} \frac{1}{n}\sum_i (y_i-x'_i \beta)^2
\]

is in the ~~Lagrangian~~ form

\[
\brl = \arg \min_{ \beta} \frac{1}{n}\sum_i (y_i-x'_i \beta)^2 + \lambda || \beta ||_2^2.
\]

For every $\lambda$ there is a unique $t$ (and vice versa) that makes 
\[
\brt = \brl
\]

Observe:

* $\lambda = 0$ (or $t = \infty$) makes $\brl = \bls$
* Any $\lambda > 0$ (or $t <\infty$)  penalizes larger values of $\beta$, effectively shrinking them.


Note: $\lambda$ and $t$ are known as ~~tuning parameters~~

## Ridge regression path

```{r load-data, echo=FALSE}
prostate = read.table('gfx/prostate.data', header=TRUE)
Y = prostate$lpsa
X = as.matrix(prostate[,names(prostate)!=c('lpsa','train')])
n = length(Y)
p = ncol(X)

library(glmnet)
ridge = glmnet(x=X,y=Y,alpha=0)
df = data.frame(as.matrix(t(ridge$beta)))
df$lambda = ridge$lambda
gather(df, key='predictor',value='coefficient',-lambda) %>%
  ggplot(aes(x=lambda,y=coefficient,color=predictor)) + geom_path() + 
  scale_x_log10() + scale_color_brewer(palette = 'Set1')
```

# Regularization and rescaling

## Least squares is invariant to rescaling

Let's multiply our design matrix by a factor of 10 to get $\widetilde\X = 10\X$.  

Then:
\[
\widetilde\beta_{\textrm{ls}} = (\widetilde\X^{\top} \widetilde\X)^{-1} \widetilde\X^{\top} Y = \frac{1}{10}(\widetilde\X^{\top} \widetilde\X)^{-1} \widetilde\X^{\top} Y = 
\frac{\widehat\beta_{\textrm{ls}}}{10}
\]

So, multiplying our data by ten just results in our estimates being reduced by one tenth.  
Hence, any prediction is left unchanged:
\[
\widetilde\X\widetilde\beta_{\textrm{ls}} = \X \bls
\]

This means, for instance, if we have a covariate measured in miles, then we will get the "same" answer if we change it to kilometers

## Least squares is invariant to rescaling: example

```{r}
n = 20
set.seed(2018-04-10)
X = matrix(runif(2*n,0,1), ncol=2)
Y = X %*% c(.5,1.5) + rnorm(n,0,.25)
Xtilde   = 2*X
Ytilde   = Y - mean(Y)
summary(lm(Y~X))$coefficients
summary(lm(Y~Xtilde))$coefficients
summary(lm(Ytilde~Xtilde))$coefficients
```

## Ridge regression (and other regularized methods) is not

```{r}
library(MASS)
lm.ridge(Y~X, lambda=1)$coef
lm.ridge(Y~Xtilde, lambda=1)$coef
lm.ridge(Ytilde~Xtilde, lambda=1)$coef
```

* `lm.ridge` automatically scales every column of $\X$ to have mean zero and Euclidean norm 1.

* It also centers $Y$.

* Together, this means there is no intercept. (We don't penalize the intercept)

* In `R`: `scale(X)` defaults to mean 0, SD 1. But you can change either.

* Another version is in the package `glmnet`. More on this in a bit.

## Solving the minimization

* One nice thing about ridge regression is that it has a closed-form solution (like OLS)

\[
\brl = (\X'\X + \lambda I)^{-1}\X'Y
\]

* This is easy to calculate in `R` for any $\lambda$.

* However, computations and interpretation are simplified if we examine the Singular Value Decomposition of $\X = UDV'$.

* Then,

\[
\brl = (\X'\X + \lambda I)^{-1}\X'Y = (VD^2V' + \lambda I)^{-1}VDU'Y 
= V(D^2+\lambda I)^{-1} DU'Y.
\]

* For computations, now we only need to invert a diagonal matrix.

* For interpretations, we can compare this to OLS:

\[
\bls = (\X'\X)^{-1}\X'Y = (VD^2V')^{-1}VDU'Y = VD^{-2}DU'Y = VD^{-1}U'Y
\]

* Notice that $\bls$ depends on $d_j/d_j^2$ while $\brl$ depends on $d_j/(d_j^2 + \lambda)$.

* Ridge regression makes the coefficients smaller relative to OLS.

* But if $\X$ has small singular values, ridge regression compensates with $\lambda$ in the denominator.

## Ridge regression and multicollinearity

Multicollinearity is a phenomenon in which a combination of predictor variables is extremely similar to another predictor variable. Some comments:

* A better phrase that is sometimes used is "$\X$ is ill-conditioned"
* It means that one of its columns is nearly (or exactly) a linear combination of other columns.  This is sometimes known as "(numerically) rank-deficient".
* If $\X = U D V'$ is ill-conditioned, then some elements of $D$ are nearly zero
* If we form $\bls= V D^{-1} U' Y$, then we see that the small
entries of $D$ are now huge (due to the inverse).  This in turn creates a huge variance.
* Recall: $\mathbb{V} \bls =  \sigma^2(\X'\X)^{-1} = \sigma^2 V D^{-2} V'$


Ridge Regression fixes this problem by preventing the division by a near zero number

Conclusion: $(\X^{\top}\X)^{-1}$ can be really unstable, while $(\X^{\top}\X + \lambda I)^{-1}$ is not.

## Can we get the best of both worlds?

To recap:

* Deciding which predictors to include, adding quadratic terms, or interactions is ~~model selection~~.

* Ridge regression provides regularization, which trades off bias and variance and also stabilizes multicollinearity.  


Ridge regression: $\min ||\mathbb{Y}-\X\beta||_2^2 \textrm{ subject to } ||\beta||_2^2 \leq t$ 

Best linear regression model: $\min || \mathbb{Y}-\X\beta||_2^2 \textrm{ subject to } ||\beta||_0 \leq t$

$||\beta||_0$ is the number of nonzero elements in $\beta$

Finding the best linear model  is a nonconvex optimization problem (In fact, it is NP-hard)

Ridge regression is convex (easy to solve), but doesn't do model selection

Can we somehow "interpolate" to get both?

## Geometry of convexity

```{r,echo=FALSE}
nbs = list()
nbs[[1]] = normBall(0,1)
qs = c(.5,.75,1,1.5,2)
for(ii in 2:6) nbs[[ii]] = normBall(qs[ii-1])
nbs = bind_rows(nbs)
nbs$lab = factor(nbs$lab, levels = unique(nbs$lab))
seg = data.frame(lab=levels(nbs$lab)[1],
                 x0=c(-1,0),x1=c(1,0),y0=c(0,-1),y1=c(0,1))
levels(seg$lab) = levels(nbs$lab)
ggplot(nbs, aes(x,y)) + geom_path(size=1.2) + facet_wrap(~lab,labeller = label_parsed) + 
  geom_segment(data=seg,aes(x=x0,xend=x1,y=y0,yend=y1),size=1.2) + 
  coord_equal() + geom_vline(xintercept = 0,size=.5) + 
  geom_hline(yintercept = 0,size=.5) +
  theme(strip.text.x = element_text(size = 12))
```

## The best of both worlds

```{r, echo=FALSE}
nb = normBall(1)
ed = ellipseData()
bols = data.frame(x=1,y=1)
bhat = lballmax(ed, 1)
ggplot(nb,aes(x,y)) + xlim(-2,2) + ylim(-2,2) + geom_path(color=red) + 
  geom_contour(mapping=aes(z=z), color=blue, data=ed, bins=7) +
  geom_vline(xintercept = 0) + geom_hline(yintercept = 0) + 
  geom_point(data=bols) + coord_equal() +
  geom_label(data=bols, mapping=aes(label=bquote('hat(beta)[ls]')), parse=TRUE,
             nudge_x = .3, nudge_y = .3) +
  geom_point(data=bhat) + xlab(expression(beta[1])) + ylab(expression(beta[2])) + 
  geom_label(data=bhat, mapping=aes(label=bquote('hat(beta)[lt]')), parse=TRUE,
             nudge_x = -.4, nudge_y = -.4)
```

This regularization set...

* ... is convex (computationally efficient)
* ... has corners (performs model selection)


# The lasso

## $\ell_1$-regularized regression

Known as 

* "lasso"
* "basis pursuit"

The estimator satisfies
\[
\blt = \arg\min_{ ||\beta||_1 \leq t}  ||\mathbb{Y}-\X\beta||_2^2 
\]

In its corresponding Lagrangian dual form:
\[
\bll = \arg\min_{\beta} ||\mathbb{Y}-\X\beta||_2^2 + \lambda ||\beta||_1
\]


## Lasso

While the ridge solution can be easily computed
\[
\brl = \arg\min_{\beta} ||\mathbb{Y}-\X\beta||_2^2 + \lambda ||\beta||_2^2 = (\X^{\top}\X + \lambda I)^{-1} \X^{\top}Y
\]

the lasso solution

\[
\bll = \arg\min_{\beta} ||\mathbb{Y}-\X\beta||_2^2 + \lambda ||\beta||_1 = \; ??
\]
doesn't have a closed form solution.


However, because the optimization problem is convex, there exist efficient algorithms for computing it

## Coefficient path: ridge vs lasso

```{r,echo=FALSE}
library(lars)
Y = prostate$lpsa
X = as.matrix(prostate[,names(prostate)!=c('lpsa','train')])
lasso = glmnet(x=X,y=Y)
df1 = data.frame(as.matrix(t(lasso$beta)))
df1$lambda = lasso$lambda
df$method = 'ridge'
df1$method = 'lasso'
out = bind_rows(df,df1)
gather(out, key='predictor',value='coefficient',-lambda,-method) %>%
  ggplot(aes(x=lambda,y=coefficient,color=predictor)) + geom_path() + 
  facet_wrap(~method,scales = 'free_x') + 
  scale_x_log10() + scale_color_brewer(palette = 'Set1')
```

## Packages

There are two main `R` implementations for finding lasso


* Using `glmnet`: `lasso.out = glmnet(X, Y, alpha=1)`.  
* Setting `alpha=0` gives ridge regression (as does `lm.ridge` in the `MASS` package)
* Setting `alpha` $\in (0,1)$ gives a method called the "elastic net" which combines ridge regression and lasso.
* Alternatively, there is  `lars`: `lars.out = lars(X, Y)`
* `lars` also other things called "Least angle", "forward stepwise", and "forward stagewise" regression

## Two packages

1. `lars` (this is the first one)
2. `glmnet` (this one is faster)

These use different algorithms, but both compute the ~~path~~ for a range of $\lambda$.

`lars` starts with an empty model and adds coefficients until saturated. The sequence of $\lambda$'s comes from the nature of the optimization problem.

`glmnet` starts with an empty model and examines each value of $\lambda$ using previous values as "warm starts". It is generally much faster than `lars` and uses lots of other tricks (as well as compiled code) for extra speed.

The path returned by `lars` as more useful than that returned by `glmnet`.

## Lasso paths

```{r}
lasso = glmnet(X,Y)
lars.out = lars(X,Y)
par(mfrow=c(1,2))
plot(lasso)
plot(lars.out,main='')
```

# Model selection

## Choosing the lambda

* You have to choose $\lambda$ in lasso or in ridge regression
* lasso selects a model (by setting coefficients to zero), but the value of $\lambda$ determines how many/which.
* All of these packages come with CV built in.
* However, the way to do it differs from package to package (whomp whomp)

## Ridge regression, `lm.ridge` version


```{r}
par(mfrow=c(1,1))
# 1. Estimate the model (note, this uses a formula, and you must supply lambda)
ridge.out = lm.ridge(lpsa~.-train, data=prostate, lambda = 0:100)
# 2. Plot it
plot(ridge.out)
# (2a). If you chose lambda poorly, this will look bad, try again
# 3. Choose lambda using GCV
plot(ridge.out$lambda,ridge.out$GCV,ty='l')
# 4. If there's a minimum, FIND IT, else try again
best.lam = which.min(ridge.out$GCV)
# 5. Return the coefs/predictions for the best model
coefs = coefficients(ridge.out)[best.lam,]
preds = as.matrix(dplyr::select(prostate,-lpsa,-train)) %*% coefs[-1] + coefs[1]
```

## `glmnet` version (lasso or ridge)



```{r}
# 1. Estimate cv and model at once, no formula version
lasso.glmnet = cv.glmnet(X,Y)
# 2. Plot the coefficient path
plot(lasso.glmnet$glmnet.fit) # the glmnet.fit == glmnet(X,Y)
# 3. Choose lambda using CV
plot(lasso.glmnet) #a different plot method for the cv fit
# 4. If the dashed lines are at the boundaries, redo it with a better set of lambda
best.lam = lasso.glmnet$lambda.min # the value, not the location (or lasso$lambda.1se)
# 5. Return the coefs/predictions for the best model
coefs.glmnet = coefficients(lasso.glmnet, s = best.lam)
preds.glmnet = predict(lasso.glmnet, newx = X, s = best.lam) # must supply `newx`
```

## `lars` version

This is incredibly difficult to cross-validate. 

The path changes from fold to fold, so things can get hairy.

In principle, the following should work.



```{r}
# 1. Estimate cv, no formula version
lasso.lars.cv = cv.lars(X,Y) # also plots it
# 2. Choose lambda using CV
best.lam.lars = lasso.lars.cv$index[which.min(lasso.lars.cv$cv)] # the location, not the value
# 3. Estimate the lasso and plot
lasso.lars = lars(X,Y) # still the whole path
# 5. Return the coefs/predictions for the best model
coefs.lars = coefficients(lasso.lars, s = best.lam.lars, mode='fraction') # annoying mode argument is required
preds.lars = predict(lasso.lars, newx=X, s = best.lam.lars, mode='fraction') # must supply `newx`
```

## Paths with chosen lambda (lasso and ridge)

```{r}
ridge.glmnet = cv.glmnet(X,Y,alpha=0,lambda.min.ratio=1e-10) # added to get a minimum
par(mfrow=c(1,2))
plot(ridge.glmnet)
plot(lasso.glmnet)
best.lam.ridge = ridge.glmnet$lambda.min
plot(ridge.glmnet$glmnet.fit,xvar='lambda', main='Ridge (glmnet)')
abline(v=log(best.lam.ridge))
plot(lasso.glmnet$glmnet.fit,xvar='lambda', main='Lasso (glmnet)')
abline(v=log(best.lam))
```